{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python -m pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import adad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, RocCurveDisplay\n",
    "import os\n",
    "import adad\n",
    "from adad.utils import to_json, open_json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from adad.distance import DAIndexGamma\n",
    "import numpy as np\n",
    "from adad.evaluate import sensitivity_specificity, cumulative_accuracy, roc_ad, permutation_auc, predictiveness_curves\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd08a5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCLF(dataset, train_split, clf, name, path=None, scale=True):\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get train and test\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy evaluation of {type(clf).__name__} for {name} dataset:\")\n",
    "    print(\"===============================================================\")\n",
    "    print(f\"Accuracy of training split: {accuracy_score(y_train, train_pred):.3f}\")\n",
    "    print(f\"Accuracy of testing split : {accuracy_score(y_test, test_pred):.3f}\\n\")\n",
    "    \n",
    "    #Save classifier\n",
    "    filename = f\"{type(clf).__name__ }_{name}\"\n",
    "    with open(os.path.join(path, filename + \".pickle\"), 'wb') as file:\n",
    "        pickle.dump(clf, file)\n",
    "    \n",
    "    #Get JSON file\n",
    "    parameter_json = clf.get_params()\n",
    "    to_json(parameter_json, os.path.join(path, filename + \".json\"))\n",
    "    \n",
    "    json_file = open_json(os.path.join(path, filename + \".json\"))\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464897e",
   "metadata": {},
   "source": [
    "### Stores all evaluations for all cv into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    evaluation = pd.DataFrame(columns=['sensitivity', 'specificity', 'cumulative_acc', 'roc_ad', 'auc(roc_ad)','permutation_auc', \n",
    "                                       'permutation_list', 'predictiveness_curves'])\n",
    "    measure = pd.DataFrame()\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        value_dict = {'sensitivity': np.round(sensitivity, 6), 'specificity': np.round(specificity, 6), \n",
    "                      'cumulative_acc': list((np.around(cumulative_acc, 6), np.around(cumulative_rate, 6))),\n",
    "                      'roc_ad': list((np.around(fpr, 6), np.around(tpr, 6))), 'auc(roc_ad)': auc_roc, \n",
    "                      'permutation_auc': auc_signi, 'permutation_list': [auc_perm],\n",
    "                      'predictiveness_curves': list((np.around(percentile, 6), np.around(err_rate, 6)))}\n",
    "        \n",
    "        evaluation = evaluation.append(value_dict, ignore_index=True)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    evaluation.to_csv(os.path.join(path, f'{name}_evaluation.csv'), index=False)\n",
    "    \n",
    "    return evaluation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a17a7c",
   "metadata": {},
   "source": [
    "### One DataFrame for each cv evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    measure = pd.DataFrame()\n",
    "    evaluation = pd.DataFrame()\n",
    "    evaluations = []\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        column1 = pd.DataFrame([np.round(sensitivity, 6)], columns=['sensitivity'])\n",
    "        column2 = pd.DataFrame([np.round(specificity, 6)],columns=['specificity'])\n",
    "        column3 = pd.DataFrame(np.around(cumulative_acc, 6),columns=['cumulative_acc'])\n",
    "        column4 = pd.DataFrame(np.around(cumulative_rate, 6), columns=['cumulative_rate'])\n",
    "        column5 = pd.DataFrame(np.around(fpr, 6),columns=['roc_ad_fpr'])\n",
    "        column6 = pd.DataFrame(np.around(tpr, 6),columns=['roc_ad_tpr'])\n",
    "        column7 = pd.DataFrame([np.round(auc_roc,6)] ,columns=['auc(roc_ad)'])\n",
    "        column8 = pd.DataFrame([np.round(auc_signi, 6)],columns=['permutation_auc'])\n",
    "        column9 = pd.DataFrame(np.around(auc_perm,6),columns=['permutation_list'])\n",
    "        column10 = pd.DataFrame(np.around(percentile, 6),columns=['pred_curve_percent'])\n",
    "        column11 = pd.DataFrame(np.around(err_rate, 6),columns=['pred_curve_err'])\n",
    "        \n",
    "        evaluation = pd.concat([column1, column2, column3, column4, column5, column6, column7, column8, column9, column10, column11],axis=1)\n",
    "        evaluation.to_csv(os.path.join(path, f'{name}_cv{col+1}_evaluation.csv'), index=False)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ffd41",
   "metadata": {},
   "source": [
    "### One DataFrame per Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90518d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    measure = pd.DataFrame()\n",
    "    single_value = pd.DataFrame()\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        column1 = pd.DataFrame([np.round(sensitivity, 6)], columns=['sensitivity'])\n",
    "        column2 = pd.DataFrame([np.round(specificity, 6)],columns=['specificity'])\n",
    "        column3 = pd.DataFrame(np.around(cumulative_acc, 6),columns=['cumulative_acc'])\n",
    "        column4 = pd.DataFrame(np.around(cumulative_rate, 6), columns=['cumulative_rate'])\n",
    "        column5 = pd.DataFrame(np.around(fpr, 6),columns=['roc_ad_fpr'])\n",
    "        column6 = pd.DataFrame(np.around(tpr, 6),columns=['roc_ad_tpr'])\n",
    "        column7 = pd.DataFrame([np.round(auc_roc,6)] ,columns=['auc(roc_ad)'])\n",
    "        column8 = pd.DataFrame([np.round(auc_signi, 6)],columns=['permutation_auc'])\n",
    "        column9 = pd.DataFrame(np.around(auc_perm,6),columns=['permutation_list'])\n",
    "        column10 = pd.DataFrame(np.around(percentile, 6),columns=['pred_curve_percent'])\n",
    "        column11 = pd.DataFrame(np.around(err_rate, 6),columns=['pred_curve_err'])\n",
    "        \n",
    "        evaluation = pd.concat([column1, column2, column3, column4, column5, column6, column7, column8, column9, column10, column11],axis=1)\n",
    "        evaluation.to_csv(os.path.join(path, f'{name}_cv{col+1}_evaluation.csv'), index=False)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d89934",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35960f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "files_path = os.path.join(parent_path, 'data', 'maccs')\n",
    "dataset_files = [os.path.join(files_path, file) for file in os.listdir(files_path)]\n",
    "\n",
    "cv_path = os.path.join(parent_path, 'experiments', 'preprocessing')\n",
    "cv_files = [os.path.join(cv_path, file) for file in os.listdir(cv_path)]\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "cv_split = pd.read_csv(cv, header=None, dtype={'id': int})\n",
    "\n",
    "results_path = os.path.join(parent_path, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aacbef",
   "metadata": {},
   "source": [
    "### Select dataset by Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b202cb",
   "metadata": {},
   "source": [
    "    Ames      |    0\n",
    "    BBBP      |    1\n",
    "    Cancer    |    2\n",
    "    CYP1A2    |    3\n",
    "    FXa       |    4\n",
    "    hERG      |    5\n",
    "    HIV       |    6\n",
    "    Liver     |    7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7021d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 3\n",
    "names_list = ['Ames', 'BBBP', 'Cancer', 'CYP1A2', 'FXa', 'hERG', 'HIV', 'Liver']\n",
    "filename = dataset_files[index]\n",
    "cv = cv_files[index]\n",
    "name = names_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de772597",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = trainCLF(dataset, cv_split, rfc, name, path=results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88099fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = DAIndexGamma(clf=rfc)\n",
    "evaluation = runAD(dataset, cv_split, rfc, ad, name+\"s\", path=results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91547ef6",
   "metadata": {},
   "source": [
    "## Using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d97b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get train and test\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy evaluation of {type(clf).__name__} for {name} dataset:\")\n",
    "    print(\"===============================================================\")\n",
    "    print(f\"Accuracy of training split: {accuracy_score(y_train, train_pred):.3f}\")\n",
    "    print(f\"Accuracy of testing split : {accuracy_score(y_test, test_pred):.3f}\\n\")\n",
    "    \n",
    "    ad.fit(X_train)\n",
    "        \n",
    "    #Gather scores and save them in csv\n",
    "    dist_measure = ad.measure(X_test)\n",
    "    new_col = pd.DataFrame(dist_measure)\n",
    "\n",
    "    #Start gathering data from evaluation functions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "    cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "    fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "    auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "    percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "\n",
    "    #Compare AUCs\n",
    "    print(f\"permutationAUC vs auc(roc_ad) of {name}:\")\n",
    "    print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "\n",
    "\n",
    "    #Create graphs to save for each split\n",
    "    fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "    axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "    axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "    roc_display.plot(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"AUC ROC\")\n",
    "    axes[1, 0].plot(percentile, err_rate)\n",
    "    axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "    plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "    plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "    fig.delaxes(axes[1, 1])\n",
    "    fig.suptitle(f'Graphs of {name}', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{os.path.join(path, f\"{name}_graphs.png\")}', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(dataset, cv_split, rfc, ad, name, path=results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
