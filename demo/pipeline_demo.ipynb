{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e43b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import adad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, RocCurveDisplay\n",
    "import os\n",
    "import adad\n",
    "from adad.utils import to_json, open_json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from adad.distance import DAIndexGamma\n",
    "import numpy as np\n",
    "from adad.evaluate import sensitivity_specificity, cumulative_accuracy, roc_ad, permutation_auc, predictiveness_curves\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd08a5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976dcbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCLF(dataset, train_split, clf, name, path=None, scale=True):\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get train and test\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy evaluation of {type(clf).__name__} for {name} dataset:\")\n",
    "    print(\"===============================================================\")\n",
    "    print(f\"Accuracy of training split: {accuracy_score(y_train, train_pred):.3f}\")\n",
    "    print(f\"Accuracy of testing split : {accuracy_score(y_test, test_pred):.3f}\\n\")\n",
    "    \n",
    "    #Save classifier\n",
    "    filename = f\"{type(clf).__name__ }_{name}\"\n",
    "    with open(os.path.join(path, filename + \".pickle\"), 'wb') as file:\n",
    "        pickle.dump(clf, file)\n",
    "    \n",
    "    #Get JSON file\n",
    "    parameter_json = clf.get_params()\n",
    "    to_json(parameter_json, os.path.join(path, filename + \".json\"))\n",
    "    \n",
    "    json_file = open_json(os.path.join(path, filename + \".json\"))\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464897e",
   "metadata": {},
   "source": [
    "### Stores all evaluations for all cv into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03edcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    evaluation = pd.DataFrame(columns=['sensitivity', 'specificity', 'cumulative_acc', 'roc_ad', 'auc(roc_ad)','permutation_auc', \n",
    "                                       'permutation_list', 'predictiveness_curves'])\n",
    "    measure = pd.DataFrame()\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        value_dict = {'sensitivity': np.round(sensitivity, 6), 'specificity': np.round(specificity, 6), \n",
    "                      'cumulative_acc': list((np.around(cumulative_acc, 6), np.around(cumulative_rate, 6))),\n",
    "                      'roc_ad': list((np.around(fpr, 6), np.around(tpr, 6))), 'auc(roc_ad)': auc_roc, \n",
    "                      'permutation_auc': auc_signi, 'permutation_list': [auc_perm],\n",
    "                      'predictiveness_curves': list((np.around(percentile, 6), np.around(err_rate, 6)))}\n",
    "        \n",
    "        evaluation = evaluation.append(value_dict, ignore_index=True)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    evaluation.to_csv(os.path.join(path, f'{name}_evaluation.csv'), index=False)\n",
    "    \n",
    "    return evaluation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a17a7c",
   "metadata": {},
   "source": [
    "### One DataFrame for each cv evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c110fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    measure = pd.DataFrame()\n",
    "    evaluation = pd.DataFrame()\n",
    "    evaluations = []\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        column1 = pd.DataFrame([np.round(sensitivity, 6)], columns=['sensitivity'])\n",
    "        column2 = pd.DataFrame([np.round(specificity, 6)],columns=['specificity'])\n",
    "        column3 = pd.DataFrame(np.around(cumulative_acc, 6),columns=['cumulative_acc'])\n",
    "        column4 = pd.DataFrame(np.around(cumulative_rate, 6), columns=['cumulative_rate'])\n",
    "        column5 = pd.DataFrame(np.around(fpr, 6),columns=['roc_ad_fpr'])\n",
    "        column6 = pd.DataFrame(np.around(tpr, 6),columns=['roc_ad_tpr'])\n",
    "        column7 = pd.DataFrame([np.round(auc_roc,6)] ,columns=['auc(roc_ad)'])\n",
    "        column8 = pd.DataFrame([np.round(auc_signi, 6)],columns=['permutation_auc'])\n",
    "        column9 = pd.DataFrame(np.around(auc_perm,6),columns=['permutation_list'])\n",
    "        column10 = pd.DataFrame(np.around(percentile, 6),columns=['pred_curve_percent'])\n",
    "        column11 = pd.DataFrame(np.around(err_rate, 6),columns=['pred_curve_err'])\n",
    "        \n",
    "        evaluation = pd.concat([column1, column2, column3, column4, column5, column6, column7, column8, column9, column10, column11],axis=1)\n",
    "        evaluation.to_csv(os.path.join(path, f'{name}_cv{col+1}_evaluation.csv'), index=False)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ffd41",
   "metadata": {},
   "source": [
    "### One DataFrame per Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a9db6",
   "metadata": {},
   "source": [
    "#TODO\n",
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    #set up ad, scaler, and path\n",
    "    ad.clf = clf\n",
    "    scaler = StandardScaler()\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get X and y\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    #Prepare DataFrames for evaluation and score data\n",
    "    measure = pd.DataFrame(columns = ['sensitivity', 'specificity', 'auc(roc_ad)', 'permutation_auc'])\n",
    "    single_value = pd.DataFrame()\n",
    "    cumulative = pd.DataFrame()\n",
    "    auc_roc = pd.DataFrame()\n",
    "    predictiveness = pd.DataFrame()\n",
    "    \n",
    "    evaluations = []\n",
    "    \n",
    "    #Test ad for all cv\n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            \n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD and classifier\n",
    "        ad.clf.fit(X_train, y_train)\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = ad.clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(\"==================================================================\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        column1 = pd.DataFrame([np.round(sensitivity, 6)], columns=['sensitivity'])\n",
    "        column2 = pd.DataFrame([np.round(specificity, 6)],columns=['specificity'])\n",
    "        column3 = pd.DataFrame(np.around(cumulative_acc, 6),columns=['cumulative_acc'])\n",
    "        column4 = pd.DataFrame(np.around(cumulative_rate, 6), columns=['cumulative_rate'])\n",
    "        column5 = pd.DataFrame(np.around(fpr, 6),columns=['roc_ad_fpr'])\n",
    "        column6 = pd.DataFrame(np.around(tpr, 6),columns=['roc_ad_tpr'])\n",
    "        column7 = pd.DataFrame([np.round(auc_roc,6)] ,columns=['auc(roc_ad)'])\n",
    "        column8 = pd.DataFrame([np.round(auc_signi, 6)],columns=['permutation_auc'])\n",
    "        column9 = pd.DataFrame(np.around(auc_perm,6),columns=['permutation_list'])\n",
    "        column10 = pd.DataFrame(np.around(percentile, 6),columns=['pred_curve_percent'])\n",
    "        column11 = pd.DataFrame(np.around(err_rate, 6),columns=['pred_curve_err'])\n",
    "        \n",
    "        evaluation = pd.concat([column1, column2, column3, column4, column5, column6, column7, column8, column9, column10, column11],axis=1)\n",
    "        evaluation.to_csv(os.path.join(path, f'{name}_cv{col+1}_evaluation.csv'), index=False)\n",
    "        evaluations.append(evaluation)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    \n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d89934",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35960f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "files_path = os.path.join(parent_path, 'data', 'maccs')\n",
    "dataset_files = [os.path.join(files_path, file) for file in os.listdir(files_path)]\n",
    "\n",
    "cv_path = os.path.join(parent_path, 'experiments', 'preprocessing')\n",
    "cv_files = [os.path.join(cv_path, file) for file in os.listdir(cv_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde20a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aacbef",
   "metadata": {},
   "source": [
    "### Select dataset by Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b202cb",
   "metadata": {},
   "source": [
    "    Ames      |    0\n",
    "    BBBP      |    1\n",
    "    Cancer    |    2\n",
    "    CYP1A2    |    3\n",
    "    FXa       |    4\n",
    "    hERG      |    5\n",
    "    HIV       |    6\n",
    "    Liver     |    7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7021d2a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\sarah\\\\Desktop\\\\applicabilityDomain\\\\data\\\\maccs\\\\cv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30540/3096031321.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnames_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcv_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\sarah\\\\Desktop\\\\applicabilityDomain\\\\data\\\\maccs\\\\cv'"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "names_list = ['Ames', 'BBBP', 'Cancer', 'CYP1A2', 'FXa', 'hERG', 'HIV', 'Liver']\n",
    "filename = dataset_files[index]\n",
    "cv = cv_files[index]\n",
    "name = names_list[index]\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "cv_split = pd.read_csv(cv, header=None, dtype={'id': int})\n",
    "\n",
    "results_path = os.path.join(parent_path, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de772597",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = trainCLF(dataset, cv_split, rfc, name, path=results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88099fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = DAIndexGamma(clf=rfc)\n",
    "evaluation = runAD(dataset, cv_split, rfc, ad, name+\"s\", path=results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91547ef6",
   "metadata": {},
   "source": [
    "## Using train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d97b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get train and test\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy evaluation of {type(clf).__name__} for {name} dataset:\")\n",
    "    print(\"===============================================================\")\n",
    "    print(f\"Accuracy of training split: {accuracy_score(y_train, train_pred):.3f}\")\n",
    "    print(f\"Accuracy of testing split : {accuracy_score(y_test, test_pred):.3f}\\n\")\n",
    "    \n",
    "    ad.fit(X_train)\n",
    "        \n",
    "    #Gather scores and save them in csv\n",
    "    dist_measure = ad.measure(X_test)\n",
    "    new_col = pd.DataFrame(dist_measure)\n",
    "\n",
    "    #Start gathering data from evaluation functions\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "    cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "    fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "    auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "    percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "    auc_roc = auc(fpr, tpr)\n",
    "\n",
    "    #Compare AUCs\n",
    "    print(f\"permutationAUC vs auc(roc_ad) of {name}:\")\n",
    "    print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "\n",
    "\n",
    "    #Create graphs to save for each split\n",
    "    fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "    axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "    axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "    roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "    roc_display.plot(ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(\"AUC ROC\")\n",
    "    axes[1, 0].plot(percentile, err_rate)\n",
    "    axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "    plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "    plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "    fig.delaxes(axes[1, 1])\n",
    "    fig.suptitle(f'Graphs of {name}', fontsize=16)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'{os.path.join(path, f\"{name}_graphs.png\")}', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(dataset, cv_split, rfc, ad, name, path=results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d03e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
