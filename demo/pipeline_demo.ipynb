{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python -m pip install .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import adad\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, auc, RocCurveDisplay\n",
    "import os\n",
    "import adad\n",
    "from adad.utils import to_json, open_json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from adad.distance import DAIndexGamma\n",
    "import numpy as np\n",
    "from adad.evaluate import sensitivity_specificity, cumulative_accuracy, roc_ad, permutation_auc, predictiveness_curves\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da3328",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCLF(dataset, train_split, clf, name, path=None, scale=True):\n",
    "    if path is None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    #Get train and test\n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #Train classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_pred = clf.predict(X_train)\n",
    "    test_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(f\"Accuracy evaluation of {type(clf).__name__} for {name} dataset:\")\n",
    "    print(\"===============================================================\")\n",
    "    print(f\"Accuracy of training split: {accuracy_score(y_train, train_pred):.3f}\")\n",
    "    print(f\"Accuracy of testing split : {accuracy_score(y_test, test_pred):.3f}\\n\")\n",
    "    \n",
    "    #Save classifier\n",
    "    filename = f\"{type(clf).__name__ }_{name}\"\n",
    "    with open(os.path.join(path, filename + \".pickle\"), 'wb') as file:\n",
    "        pickle.dump(clf, file)\n",
    "    \n",
    "    #Get JSON file\n",
    "    parameter_json = clf.get_params()\n",
    "    to_json(parameter_json, os.path.join(path, filename + \".json\"))\n",
    "    \n",
    "    json_file = open_json(os.path.join(path, filename + \".json\"))\n",
    "    return json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edcc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAD(dataset, cv_split, clf, ad, name, path=None, scale=True):\n",
    "    if path == None:\n",
    "        path = os.getcwd()\n",
    "    \n",
    "    X = dataset.iloc[:, :-1].values\n",
    "    y = dataset.iloc[:, -1].values\n",
    "    size = X.shape[0]\n",
    "    \n",
    "    evaluation = pd.DataFrame(columns=['sensitivity', 'specificity', 'cumulative_acc', 'roc_ad', 'auc(roc_ad)','permutation_auc', \n",
    "                                       'permutation_list', 'predictiveness_curves'])\n",
    "    measure = pd.DataFrame()\n",
    "    \n",
    "    for col in cv_split:\n",
    "        #Find indexes\n",
    "        train_idx = cv_split[col].to_numpy()\n",
    "        train_idx = train_idx[~np.isnan(train_idx)].astype(int)\n",
    "        \n",
    "        all_idx = np.arange(0, size)\n",
    "        bool_train = np.isin(all_idx, train_idx)\n",
    "        test_idx = all_idx[~bool_train]\n",
    "        \n",
    "        #Scale train and test datasets\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "        \n",
    "        #Train AD\n",
    "        ad.fit(X_train)\n",
    "        \n",
    "        #Gather scores and save them in csv\n",
    "        dist_measure = ad.measure(X_test)\n",
    "        new_col = pd.DataFrame(np.around(dist_measure, decimals=6), columns=[f\"cv_{col+1}\"])\n",
    "        measure = pd.concat([measure, new_col], axis=1)\n",
    "        \n",
    "        #Start gathering data from evaluation functions\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        sensitivity, specificity = sensitivity_specificity(y_test, y_pred)\n",
    "        cumulative_acc, cumulative_rate = cumulative_accuracy(y_test, y_pred, dist_measure)\n",
    "        fpr, tpr = roc_ad(y_test, y_pred, dist_measure)\n",
    "        auc_signi, auc_perm = permutation_auc(y_test, y_pred, dist_measure)\n",
    "        percentile, err_rate = predictiveness_curves(y_test, y_pred, dist_measure)\n",
    "        auc_roc = auc(fpr, tpr)\n",
    "        \n",
    "        #Compare AUCs\n",
    "        print(f\"permutationAUC vs auc(roc_ad) of {name} for split{col+1}:\")\n",
    "        print(f\"         {auc_signi:.3f} vs {auc_roc:.3f}\\n\")\n",
    "        \n",
    "        \n",
    "        #Create graphs to save for each split\n",
    "        fig, axes = plt.subplots(2,2,figsize=(6,6))\n",
    "        axes[0, 0].plot(cumulative_rate, cumulative_acc)\n",
    "        axes[0, 0].set_title(\"Cumulative Accuracy\")\n",
    "        roc_display = RocCurveDisplay(fpr=fpr, tpr=tpr,roc_auc=auc_roc)\n",
    "        roc_display.plot(ax=axes[0, 1])\n",
    "        axes[0, 1].set_title(\"AUC ROC\")\n",
    "        axes[1, 0].plot(percentile, err_rate)\n",
    "        axes[1, 0].set_title(\"Predictiveness Curves (PC)\")\n",
    "        plt.setp(axes[0, 0], xlabel='Cumulative Rate', ylabel='Cumulative Accuracy (%)')\n",
    "        plt.setp(axes[1, 0], xlabel='Percentile', ylabel='Error Rate')\n",
    "        fig.delaxes(axes[1, 1])\n",
    "        fig.suptitle(f'Graphs of {name} for split{col+1}', fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'{os.path.join(path, f\"{name}_split{col+1}_graphs.png\")}', dpi=300)\n",
    "        \n",
    "        #Save evaluation\n",
    "        value_dict = {'sensitivity': np.round(sensitivity, 6), 'specificity': np.round(specificity, 6), \n",
    "                      'cumulative_acc': list((np.around(cumulative_acc, 6), np.around(cumulative_rate, 6))),\n",
    "                      'roc_ad': list((np.around(fpr, 6), np.around(tpr, 6))), 'auc(roc_ad)': auc_roc, \n",
    "                      'permutation_auc': auc_signi, 'permutation_list': [auc_perm],\n",
    "                      'predictiveness_curves': list((np.around(percentile, 6), np.around(err_rate, 6)))}\n",
    "        \n",
    "        evaluation = evaluation.append(value_dict, ignore_index=True)\n",
    "    \n",
    "    measure.to_csv(os.path.join(path, f'{name}_scores.csv'), index=False)\n",
    "    evaluation.to_csv(os.path.join(path, f'{name}_evaluation.csv'), index=False)\n",
    "    \n",
    "    return evaluation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8fd7f6",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35960f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "files_path = os.path.join(parent_path, 'data', 'maccs')\n",
    "dataset_files = [os.path.join(files_path, file) for file in os.listdir(files_path)]\n",
    "filename = dataset_files[1]\n",
    "\n",
    "cv_path = os.path.join(parent_path, 'experiments', 'preprocessing')\n",
    "cv_files = [os.path.join(cv_path, file) for file in os.listdir(cv_path)]\n",
    "cv = cv_files[1]\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "cv_split = pd.read_csv(cv, header=None, dtype={'id': int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de772597",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "rfc = RandomForestClassifier(n_estimators=300, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c05cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json = trainCLF(dataset, cv_split, rfc, 'bbbp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d2421",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88099fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = DAIndexGamma(clf=rfc)\n",
    "measure = runAD(dataset, cv_split, rfc, ad, 'bbbp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
